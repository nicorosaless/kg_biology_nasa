# Knowledge Graph Pipeline para Biosciencia Espacial NASA
> Pipeline completo: PDF ‚Üí Entidades + Relaciones ‚Üí Grafo Multi-Escala ‚Üí Insights Accionables

---

## ‚ö†Ô∏è Estado del Proyecto

### ‚úÖ Implementado (Prototipo Funcional)
**M√≥dulo**: `kg_creator/` (c√≥digo m√≠nimo viable)

**Funcionalidad**:
- ‚úÖ Carga `.content.json` de un paper individual
- ‚úÖ Extrae entidades con heur√≠sticas regex:
  - `ORGANISM`: binomiales latinos (ej: *Homo sapiens*)
  - `SECTION_CONCEPT`: t√©rminos clave (gene, protein, microgravity, etc.)
  - `CHEMICAL`: tokens uppercase (IL, NF, ATP, RNA, etc.)
- ‚úÖ Genera relaciones `COOCCURS_WITH` por co-ocurrencia
- ‚úÖ Ingest directo a Neo4j v√≠a Bolt driver

**Uso**:
```bash
python -m kg_creator.run_kg PMC11988870 --wipe  # Procesa 1 paper ‚Üí Neo4j
```

**Resultado**: Paper ejemplo (PMC11988870) ‚Üí **164 entidades** + **6,244 relaciones**

**Exports para UI (contrato m√≠nimo)**:
- `graph_core.json` (contract): nodos y edges con s√≥lo campos esenciales + objeto de navegaci√≥n.
- `graph_vis.json`: versi√≥n derivada con layout ligero (color/posici√≥n/tama√±o) generada autom√°ticamente a partir de `graph_core.json`.
- `section_overview.json`: snapshot de secciones (nodos = secciones, edges opcionales futuro).
- `section_XX_<slug>.json`: subgrafos locales por secci√≥n (‚â§50 nodos) con el mismo esquema m√≠nimo que `graph_core.json`.

### Esquema M√≠nimo (graph_core.json / section_*.json)
```json
{
  "paper_id": "PMC11988870",
  "nodes": [
    {
      "id": 12,
      "label": "TP53",
      "type": "GENE_PRODUCT",
      "freq": 3,
      "nav": {
        "section": "RESULTS",
        "sentence_id": 145,
        "char_start": 12345,
        "char_end": 12349,
        "anchor": "PMC11988870_12345_12349"
      }
    },
    {
      "id": 47,
      "label": "apoptosis",
      "type": "PROCESS",
      "freq": 5,
      "nav": {"section": "RESULTS", "sentence_id": 146, "char_start": 12510, "char_end": 12519, "anchor": "PMC11988870_12510_12519"}
    }
  ],
  "edges": [
    {"id": 301, "source": 12, "target": 47, "type": "COOCCURS_SENT"}
  ],
  "stats": {"n_entities": 164, "n_relations": 6244}
}
```
Campos eliminados deliberadamente: `created_at`, `color`, `size`, `x`, `y`, evidencia cruda y metadatos secundarios. La UI puede recalcular layout y estilos sin afectar persistencia.

Regeneraci√≥n: `graph_vis.json` y subgrafos se pueden volver a crear ejecutando nuevamente Fase 5 sin repetir Fases 1‚Äì4 siempre que `graph_core.json` exista.

### Navegaci√≥n (anchor)
El campo `nav.anchor` permite mapear un nodo a una regi√≥n del PDF/HTML combinando `paper_id` y offsets globales: `<PMCID>_<char_start>_<char_end>`. Puede usarse como id en el visor para hacer scroll directo.

### graph_vis.json (derivado)
Incluye adem√°s de `nodes` / `edges`:
```json
{
  "nodes": [
    {"id": 12, "label": "TP53", "type": "GENE_PRODUCT", "color": "#1f77b4", "degree": 14, "frequency": 3, "size": 22.4, "x": 0.6159, "y": -0.7888}
  ],
  "edges": [ {"id": 301, "source": 12, "target": 47, "type": "COOCCURS_SENT"} ],
  "meta": {"paper_id": "PMC11988870", "visual_counts": {"entities": 40, "relations": 88}}
}
```
Se genera autom√°ticamente; si se borra puede regenerarse desde `graph_core.json`.

**Retenci√≥n m√≠nima recomendada (modo slim)**:
Si quieres ahorrar espacio por paper y s√≥lo necesitas mostrar el grafo:
1. Conservar `summary_and_content/<paper_id>.content.json` (para volver a enriquecer si hiciera falta).
2. Conservar `graph/phase5/graph_core.json` y `graph/phase5/graph_vis.json`.
3. (Opcional) Eliminar o comprimir `phase1`‚Äì`phase4` y `graph.json` completo.

Con `graph_core.json` puedes recalcular un nuevo `graph_vis.json` ajustando par√°metros (thresholds, layout) sin rehacer NER ni relaciones.

**Subgrafos por secci√≥n (actualizado)**:
- Ranking h√≠brido: `degree` ‚Üí `frequency` (configurable) para seleccionar hasta `SECTION_SUBGRAPH.max_nodes` (por defecto 50, se puede bajar a 40 si la UI lo requiere).
- Misma estructura m√≠nima que `graph_core.json`:
```json
{
  "paper_id": "PMCXXXX",
  "section": "RESULTS",
  "n_nodes": 37,
  "n_edges": 54,
  "nodes": [ {"id": 12, "label": "TP53", "type": "GENE_PRODUCT", "freq": 3, "nav": {"section": "RESULTS", "sentence_id": 145, "char_start": 12345, "char_end": 12349, "anchor": "PMCXXXX_12345_12349"}} ],
  "edges": [ {"id": 210, "type": "COOCCURS_SENT", "source": 12, "target": 45} ]
}
```

**Limitaciones**:
- ‚ùå No normaliza entidades a ontolog√≠as (UMLS/NCBI/GO)
- ‚ùå Relaciones basadas en co-ocurrencia global (no contexto oracional)
- ‚ùå Sin NER cient√≠fico (SciSpaCy/BioBERT)

---

### üîÆ Roadmap Futuro (Arquitectura `kg_pipeline/`)
El resto de este documento describe la **arquitectura objetivo** en 5 fases con:
- NER cient√≠fico + normalizaci√≥n UMLS/NCBI
- Extracci√≥n de relaciones causales (patterns + LLM)
- Grafo global multi-paper con merge inteligente
- Dashboard interactivo

**Ver carpeta `kg_creator/` para la implementaci√≥n actual minimal.**

---

## Objetivo del Proyecto
Construir un sistema automatizado de extracci√≥n y an√°lisis de Knowledge Graphs (KG) sobre **608 publicaciones biom√©dicas de NASA** para:
- **Cient√≠ficos**: Descubrir hip√≥tesis no obvias (e.g., "genes regulados por microgravedad con impacto en reparaci√≥n √≥sea").
- **Gerentes de Misi√≥n**: Priorizar experimentos cr√≠ticos (e.g., gaps en prote√≠nas de estr√©s oxidativo).
- **Dashboard Web**: Visualizaci√≥n interactiva de trends, relaciones causales y l√≠neas de investigaci√≥n emergentes.

**Dominio**: Efectos de microgravedad/radiaci√≥n espacial en organismos modelo (ratones, plantas, c√©lulas), genes, v√≠as metab√≥licas y procesos biol√≥gicos.

**Meta**: Grafo global auditado (50k-200k nodos) con evidencia textual por edge; prototipo funcional en 2 semanas para NASA Challenge.

---
## Arquitectura del Pipeline (5 Fases)

### Fase 1: Adquisici√≥n y Parsing Base
**Input**: 608 PMCIDs (CSV)  
**Output**: `processed_grobid_pdfs/<PMCID>/<PMCID>.grobid.content.json`

**Herramientas**:
- `SB_publications/download_pdfs.py`: Descarga batch via APIs (PMC/Europe PMC) con manejo de POW.
- GROBID (Docker `lfoppiano/grobid:0.8.0`): Extracci√≥n estructurada (TEI ‚Üí JSON).
- `summary/process_grobid_pdfs.py`: Orquestador batch con fallback HTTP.

**Formato JSON**:
```json
{
  "metadata": {"title": "...", "authors": [...], "pmcid": "PMC123"},
  "abstract": "...",
  "sections": [
    {"heading": "Results", "text": "...", "paragraphs": [...]},
    ...
  ],
  "figures": [...],
  "references": [...]
}
```

**Criterios de Calidad**:
- M√≠nimo 500 palabras en secciones Results/Discussion/Conclusions.
- Metadata completa (t√≠tulo, PMCID, a√±o).
- Log de fallos: `failed_pdfs.json` con error codes.

---
### Fase 2: Pre-Procesamiento y Segmentaci√≥n
**Objetivo**: Texto limpio por oraci√≥n para NER/LLM.

**M√≥dulos** (nuevo paquete `kg_pipeline/preprocessing/`):
1. **Filtro de Secciones** (`filter_sections.py`):
   - Prioridad: Abstract > Results > Discussion > Conclusions.
   - Descartar: Acknowledgements, References, Supplementary.
   - Output: ~400-800 oraciones por paper.

2. **Limpieza** (`clean_text.py`):
   - Remover citations inline `[1,2]`, tablas mal parseadas, f√≥rmulas LaTeX residuales.
   - Normalizaci√≥n Unicode (e.g., ligaduras "Ô¨Å" ‚Üí "fi").
   - Expansi√≥n de abreviaciones comunes (e.g., "e.g." ‚Üí "for example").

3. **Segmentaci√≥n Oracional** (`sentence_splitter.py`):
   - spaCy `en_core_sci_md` (especializado en bio).
   - Preserva contexto: `{"sent_id": "PMC123_sect2_sent5", "text": "...", "section": "Results"}`.
   - Ventana de 3 oraciones para co-ocurrencia contextual.

**Output**: `processed_sentences/<PMCID>.sentences.json`

---
### Fase 3: Extracci√≥n de Entidades (NER + Normalizaci√≥n)
**Objetivo**: Identificar + normalizar entidades biom√©dicas.

**Stack**:
- **NER Base**: SciSpaCy `en_ner_bc5cdr_md` (Chemical/Disease) + `en_ner_bionlp13cg_md` (Gene/Protein).
- **Normalizaci√≥n**:
  - UMLS Linker (CUI): Conceptos generales (Process, Phenomenon).
  - NCBI Gene: Genes/Prote√≠nas ‚Üí Entrez ID.
  - NCBI Taxonomy: Organismos ‚Üí TaxID.
  - ChEBI: Compuestos qu√≠micos ‚Üí ChEBI ID.

**Tipos de Entidades** (ontolog√≠a cerrada):
```python
ENTITY_TYPES = {
    'ORGANISM': {'source': 'NCBI_Taxonomy', 'id_prefix': 'taxid:'},
    'GENE': {'source': 'NCBI_Gene', 'id_prefix': 'gene:'},
    'PROTEIN': {'source': 'UniProt', 'id_prefix': 'uniprot:'},
    'CHEMICAL': {'source': 'ChEBI', 'id_prefix': 'chebi:'},
    'DISEASE': {'source': 'UMLS', 'id_prefix': 'cui:'},
    'PROCESS': {'source': 'GO', 'id_prefix': 'go:'},  # Biological Process
    'PATHWAY': {'source': 'KEGG', 'id_prefix': 'kegg:'},
    'PHENOMENON': {'custom': True, 'examples': ['microgravity', 'spaceflight', 'radiation']}
}
```

**M√≥dulo** (`kg_pipeline/ner/entity_extractor.py`):
- Input: Oraciones segmentadas.
- Output: `entities/<PMCID>.entities.jsonl` (l√≠nea por entidad):
  ```json
  {"ent_id": "E123", "text": "NF-Œ∫B", "type": "GENE", "norm_id": "gene:4790", "sent_id": "PMC123_sect2_sent5", "offset": [34, 39], "score": 0.95}
  ```

**Validaci√≥n**:
- Desambiguaci√≥n manual para t√©rminos ambiguos (e.g., "ROS" ‚Üí Reactive Oxygen Species vs Robot Operating System).
- Diccionario de stop-entities (e.g., "Figure 1", "Table S1").

---
### Fase 4: Extracci√≥n de Relaciones (Reglas + LLM H√≠brido)
**Objetivo**: Rels causales/funcionales con evidencia.

**Estrategia Multi-Nivel**:

1. **Co-Ocurrencia Filtrada** (baseline):
   - Ventana: 3 oraciones (contexto local).
   - Tipo: `COOCCURS_WITH` (peso por frecuencia).
   - Solo si tipos distintos (e.g., GENE-PROCESS, no GENE-GENE).

2. **Patrones Ling√º√≠sticos** (`kg_pipeline/relations/pattern_matcher.py`):
   - Dependency parsing (spaCy).
   - Patrones:
     ```python
     RELATION_PATTERNS = {
         'INHIBITS': ['inhibit', 'suppress', 'downregulate', 'block'],
         'ACTIVATES': ['activate', 'stimulate', 'upregulate', 'induce'],
         'REGULATES': ['regulate', 'modulate', 'control', 'affect'],
         'INTERACTS_WITH': ['interact', 'bind', 'associate', 'complex'],
         'EXPRESSED_IN': ['express', 'detected in', 'present in']
     }
     ```
   - Validaci√≥n: Sujeto y objeto deben ser entidades normalizadas.

3. **LLM-Based RE** (opcional, para oraciones complejas):
   - Modelo: BioGPT-Large / PubMedBERT fine-tuned en ChemProt/DDI corpus.
   - Prompt:
     ```
     Extract causal relations from: "{sentence}"
     Entities: {entities_in_sentence}
     Format: [Source Entity] --[RELATION_TYPE]--> [Target Entity]
     ```
   - Threshold: confianza > 0.8 para incluir.

**M√≥dulo** (`kg_pipeline/relations/relation_extractor.py`):
- Input: Entidades + oraciones.
- Output: `relations/<PMCID>.relations.jsonl`:
  ```json
  {"rel_id": "R456", "type": "INHIBITS", "source": "E123", "target": "E789", "evidence": {"sent_id": "...", "text": "NF-Œ∫B inhibits apoptosis...", "method": "pattern", "score": 0.92}}
  ```

---
### Fase 5: Construcci√≥n y Fusi√≥n del Grafo
**Objetivo**: DiGraph por paper + merge global.

**M√≥dulo** (`kg_pipeline/graph/builder.py`):

#### 5.1 Grafo Per-Paper
```python
import networkx as nx

G = nx.DiGraph()
# Nodos
for ent in entities:
    G.add_node(ent['ent_id'], 
               text=ent['text'],
               type=ent['type'],
               norm_id=ent['norm_id'],
               pmcid=pmcid,
               freq=ent_freq[ent['norm_id']])

# Edges
for rel in relations:
    G.add_edge(rel['source'], rel['target'],
               type=rel['type'],
               evidence=rel['evidence']['text'],
               sent_id=rel['evidence']['sent_id'],
               score=rel['evidence']['score'],
               method=rel['evidence']['method'])
```

**M√©tricas por Paper**:
- Nodos: 150-300 (entidades √∫nicas normalizadas).
- Edges: 200-500 (relaciones con score > threshold).
- Densidad: ~0.01-0.03 (grafos dispersos).
- Top nodos por grado: genes hub (e.g., TP53, MYC).

**Output**: `graphs/per_paper/<PMCID>.gpickle`

#### 5.2 Grafo Global (Merge)
```python
G_global = nx.DiGraph()

for pmcid_graph in all_graphs:
    for node, attrs in pmcid_graph.nodes(data=True):
        norm_id = attrs['norm_id']
        if norm_id not in G_global:
            G_global.add_node(norm_id, **attrs, papers=[pmcid])
        else:
            G_global.nodes[norm_id]['papers'].append(pmcid)
            G_global.nodes[norm_id]['freq'] += attrs['freq']
    
    for u, v, attrs in pmcid_graph.edges(data=True):
        norm_u, norm_v = get_norm_id(u), get_norm_id(v)
        edge_key = (norm_u, norm_v, attrs['type'])
        if G_global.has_edge(norm_u, norm_v):
            G_global[norm_u][norm_v]['weight'] += attrs['score']
            G_global[norm_u][norm_v]['evidences'].append(attrs)
        else:
            G_global.add_edge(norm_u, norm_v, 
                              type=attrs['type'],
                              weight=attrs['score'],
                              evidences=[attrs])
```

**Filtros Post-Merge**:
- Nodos: M√≠nimo 2 papers de soporte.
- Edges: Weight > 1.5 (agregado multi-paper).
- Top 10% por PageRank para visualizaci√≥n inicial.

**Output**: `graphs/global/global_graph.gpickle` + `global_graph_stats.json`

---
### Fase 6: Persistencia y Export
**M√≥dulos** (`kg_pipeline/export/`):

1. **Neo4j Ingest** (`neo4j_loader.py`):
   ```cypher
   CREATE INDEX norm_id_idx FOR (n:Entity) ON (n.norm_id);
   
   MERGE (e:Entity {norm_id: $norm_id})
   SET e.text = $text, e.type = $type, e.papers = $papers;
   
   MATCH (s:Entity {norm_id: $source}), (t:Entity {norm_id: $target})
   MERGE (s)-[r:RELATION {type: $rel_type}]->(t)
   SET r.weight = $weight, r.evidences = $evidences;
   ```

2. **GraphML Export** (para Gephi/Cytoscape):
   ```python
   nx.write_graphml(G_global, 'graphs/export/global.graphml')
   ```

3. **Summary JSON** (para dashboard):
   ```json
   {
     "total_nodes": 52134,
     "total_edges": 184392,
     "top_hub_genes": ["TP53", "MYC", "NFKB1"],
     "top_pathways": ["Apoptosis", "Cell Cycle", "DNA Repair"],
     "key_findings": [
       "Microgravity inhibits osteoblast differentiation via Wnt pathway (15 papers)",
       "Radiation activates p53-mediated apoptosis in lymphocytes (23 papers)"
     ]
   }
   ```

---
## Estructura de Carpetas (Reemplazo de `kg_creator`)

```
kg_pipeline/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ filter_sections.py       # Priorizaci√≥n de secciones
‚îÇ   ‚îú‚îÄ‚îÄ clean_text.py             # Limpieza + normalizaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ sentence_splitter.py      # Segmentaci√≥n oracional
‚îú‚îÄ‚îÄ ner/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ entity_extractor.py       # NER + normalizaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ umls_linker.py            # Wrapper UMLS
‚îÇ   ‚îî‚îÄ‚îÄ entity_types.py           # Ontolog√≠a cerrada
‚îú‚îÄ‚îÄ relations/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ pattern_matcher.py        # Reglas ling√º√≠sticas
‚îÇ   ‚îú‚îÄ‚îÄ llm_extractor.py          # BioGPT RE (opcional)
‚îÇ   ‚îî‚îÄ‚îÄ relation_types.py         # Taxonom√≠a relaciones
‚îú‚îÄ‚îÄ graph/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ builder.py                # Construcci√≥n DiGraph per-paper
‚îÇ   ‚îú‚îÄ‚îÄ merger.py                 # Fusi√≥n global
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py                # Stats (grado, centralidad)
‚îú‚îÄ‚îÄ export/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ neo4j_loader.py           # Ingest con driver
‚îÇ   ‚îú‚îÄ‚îÄ graphml_export.py         # Export para viz externa
‚îÇ   ‚îî‚îÄ‚îÄ summary_generator.py      # JSON resumido
‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ run_pipeline.py           # Orquestador CLI
‚îî‚îÄ‚îÄ config/
    ‚îú‚îÄ‚îÄ pipeline_config.yaml      # Par√°metros globales
    ‚îî‚îÄ‚îÄ entity_mappings.json      # Diccionarios personalizados
```

---
## Ejecuci√≥n (CLI Unificado)

```bash
# Pipeline completo (por fases)
python -m kg_pipeline.cli.run_pipeline \
  --phase all \
  --input processed_grobid_pdfs \
  --output kg_outputs \
  --config kg_pipeline/config/pipeline_config.yaml

# Solo NER + Relations
python -m kg_pipeline.cli.run_pipeline \
  --phase ner,relations \
  --input processed_sentences \
  --output entities_relations

# Build global graph + ingest Neo4j
python -m kg_pipeline.cli.run_pipeline \
  --phase graph,export \
  --neo4j-uri bolt://localhost:7687 \
  --wipe-neo4j
```

**Config Example** (`pipeline_config.yaml`):
```yaml
preprocessing:
  priority_sections: [abstract, results, discussion, conclusions]
  min_sentence_words: 5
  max_sentences_per_paper: 800

ner:
  scispacy_model: en_ner_bc5cdr_md
  umls_threshold: 0.85
  enable_disambiguation: true

relations:
  methods: [pattern, cooccur]  # llm requires API key
  pattern_threshold: 0.8
  cooccur_window: 3

graph:
  per_paper_min_nodes: 50
  global_min_papers: 2
  edge_weight_threshold: 1.5
```

---
## M√©tricas de Validaci√≥n

### Calidad de Datos (por fase)
1. **Parsing**: % PDFs con >500 palabras en secciones clave.
2. **NER**: Precision/Recall en subset anotado (gold standard: 50 papers).
3. **Relations**: Acuerdo inter-anotador (Cohen's Œ∫ > 0.7).
4. **Graph**: Densidad, componentes conectados, top-k hub consistency.

### KPIs Finales
- **Cobertura**: >90% de papers con grafo per-paper construido.
- **Consensus**: ‚â•60% de edges en grafo global soportados por ‚â•3 papers.
- **Insights**: ‚â•20 hip√≥tesis no triviales validables (e.g., genes/v√≠as nuevas).

---
## Pr√≥ximos Pasos (Roadmap 2 Semanas)

### Sprint 1 (D√≠as 1-7)
- [x] GROBID batch completo (608 PDFs).
- [ ] Pre-procesamiento + segmentaci√≥n oracional.
- [ ] NER en subset (50 papers) + validaci√≥n.
- [ ] Baseline co-ocurrencia + pattern matching.

### Sprint 2 (D√≠as 8-14)
- [ ] NER completo + normalizaci√≥n UMLS/NCBI.
- [ ] LLM RE opcional (si API disponible).
- [ ] Construcci√≥n grafos per-paper + merge global.
- [ ] Ingest Neo4j + dashboard prototipo (Streamlit).

### Mejoras Futuras
- **Active Learning**: Anotaci√≥n asistida para mejorar NER/RE.
- **Ontology Alignment**: Mapeo a GO/KEGG/Reactome.
- **Temporal Graphs**: An√°lisis de trends por a√±o.
- **LLM-RAG**: QA sobre el grafo (e.g., "genes m√°s afectados por radiaci√≥n").

---
## Recursos Clave

**Datos**:
- 608 PMCIDs: `SB_publications/SB_publication_PMC.csv`
- GROBID JSONs: `processed_grobid_pdfs/`

**Infraestructura**:
- GROBID: Docker `lfoppiano/grobid:0.8.0`
- Neo4j: `bolt://localhost:7687` (credentials en `.env`)
- Python: `3.10+` con `spacy`, `scispacy`, `networkx`, `neo4j`

**Referencias**:
- SciSpaCy: https://allenai.github.io/scispacy/
- UMLS: https://www.nlm.nih.gov/research/umls/
- BioGPT: https://github.com/microsoft/BioGPT

---
## Notas de Implementaci√≥n

**Estado Actual**: 
- ‚úÖ Fase 1 (parsing) funcional.
- ‚ö†Ô∏è Fases 2-6: Esqueleto listo, requiere implementaci√≥n.
- ‚ùå `kg_creator/` obsoleto (heur√≠sticas b√°sicas, no normalizaci√≥n).

**Decisiones de Dise√±o**:
- **Modular**: Cada fase independiente para iteraci√≥n r√°pida.
- **Auditable**: Evidencia textual + offsets en cada edge.
- **Escalable**: Procesa 1 paper/seg (‚âà10min batch completo).
- **Reproducible**: Config YAML + seeds fijos + logs estructurados.

**Riesgos**:
- Normalizaci√≥n UMLS lenta (soluci√≥n: cache local).
- LLM RE costoso (fallback: solo patterns).
- Neo4j OOM en grafos grandes (soluci√≥n: batch ingest + √≠ndices).